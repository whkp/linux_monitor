#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆAIæ™ºèƒ½å‘Šè­¦ç³»ç»Ÿæµ‹è¯•è„šæœ¬
æ— éœ€å¤–éƒ¨ä¾èµ–ï¼ŒéªŒè¯æ ¸å¿ƒåŠŸèƒ½
"""
import sys
import asyncio
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

# æ¨¡æ‹ŸMonitoringKnowledgeBaseæ¥é¿å…ChromaDBä¾èµ–
class MockMonitoringKnowledgeBase:
    def __init__(self):
        self.mock_solutions = {
            "CPU": ["æ£€æŸ¥topå‘½ä»¤æŸ¥çœ‹é«˜CPUè¿›ç¨‹", "ä¼˜åŒ–åº”ç”¨æ€§èƒ½"],
            "å†…å­˜": ["æ£€æŸ¥å†…å­˜æ³„æ¼", "é‡å¯å ç”¨å†…å­˜çš„è¿›ç¨‹"],
            "è´Ÿè½½": ["åˆ†æI/OçŠ¶æ€", "æ£€æŸ¥ç³»ç»Ÿä»»åŠ¡"]
        }
    
    def search_solutions(self, query, k=3):
        results = []
        for key, solutions in self.mock_solutions.items():
            if key in query:
                for solution in solutions[:k]:
                    results.append({'content': solution})
        return results

# æ¨¡æ‹Ÿç›‘æ§æ•°æ®
from src.models.data_models import MonitoringData

def create_test_data(scenario="normal"):
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    base_data = {
        'timestamp': datetime.now(),
        'hostname': 'test-server',
        'cpu_load_1min': 2.0,
        'cpu_load_5min': 1.8,
        'cpu_load_15min': 1.5,
        'cpu_stats': [],
        'soft_irq_stats': [],
        'memory_stats': None,
        'network_stats': [],
        'memory_total': 8 * 1024**3,  # 8GB
        'memory_used': 4 * 1024**3,   # 4GB
        'memory_available': 4 * 1024**3  # 4GB
    }
    
    if scenario == "high_cpu":
        base_data.update({
            'cpu_usage': 90.0,
            'cpu_load_1min': 8.5
        })
    elif scenario == "high_memory":
        base_data.update({
            'cpu_usage': 45.0,
            'memory_used': 7.5 * 1024**3,  # 7.5GB
            'memory_available': 0.5 * 1024**3  # 0.5GB
        })
    elif scenario == "load_cpu_mismatch":
        base_data.update({
            'cpu_usage': 30.0,
            'cpu_load_1min': 12.0
        })
    else:  # normal
        base_data.update({
            'cpu_usage': 25.0
        })
    
    return MonitoringData(**base_data)

async def test_analysis_agent():
    """æµ‹è¯•åˆ†æAgent"""
    print("=" * 60)
    print("ğŸ§ª AIæ™ºèƒ½å‘Šè­¦ç³»ç»Ÿ - ç®€åŒ–ç‰ˆæµ‹è¯•")
    print("=" * 60)
    print()
    
    # åˆ›å»ºæ¨¡æ‹ŸçŸ¥è¯†åº“
    mock_kb = MockMonitoringKnowledgeBase()
    
    # å¯¼å…¥å¹¶åˆ›å»ºAgent
    from src.agents.analysis_agent import MonitoringAgent
    agent = MonitoringAgent(mock_kb)
    
    # æµ‹è¯•åœºæ™¯
    scenarios = [
        ("æ­£å¸¸çŠ¶æ€", "normal"),
        ("CPUä½¿ç”¨ç‡è¿‡é«˜", "high_cpu"),
        ("å†…å­˜ä¸è¶³", "high_memory"),
        ("è´Ÿè½½é«˜CPUä½(I/Oç“¶é¢ˆ)", "load_cpu_mismatch")
    ]
    
    for scenario_name, scenario_type in scenarios:
        print(f"ğŸ“Š æµ‹è¯•åœºæ™¯: {scenario_name}")
        print("-" * 40)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = create_test_data(scenario_type)
        
        # æ‰§è¡Œåˆ†æ
        result = await agent.analyze(test_data)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"ä¸»æœº: {test_data.hostname}")
        print(f"CPU: {test_data.cpu_usage:.1f}%")
        print(f"å†…å­˜: {(test_data.memory_used/test_data.memory_total)*100:.1f}%")
        print(f"è´Ÿè½½: {test_data.cpu_load_1min:.1f}")
        print()
        
        if result.anomalies_detected:
            print("âš ï¸  æ£€æµ‹åˆ°é—®é¢˜:")
            for anomaly in result.anomalies_detected:
                print(f"  â€¢ {anomaly}")
            print()
            
            if result.analysis_details.get("llm_analysis"):
                print(f"ğŸ¤– AIåˆ†æ: {result.analysis_details['llm_analysis']}")
                print()
            
            if result.recommendations:
                print("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
                for i, rec in enumerate(result.recommendations, 1):
                    print(f"  {i}. {rec}")
                print()
            
            print(f"ç½®ä¿¡åº¦: {result.confidence_score:.2f}")
            print(f"LLMå¯ç”¨: {result.analysis_details.get('llm_enabled', False)}")
            print(f"LLMè§¦å‘: {result.analysis_details.get('llm_triggered', False)}")
            
            if result.analysis_details.get('fallback_used'):
                print("ğŸ”„ ä½¿ç”¨äº†é™çº§æœºåˆ¶")
        else:
            print("âœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
        
        print("=" * 60)
        print()

async def main():
    try:
        await test_analysis_agent()
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼ç®€åŒ–ç‰ˆAIæ™ºèƒ½å‘Šè­¦ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
        print()
        print("ğŸ’¡ è¯´æ˜:")
        print("  â€¢ æ— éœ€å®‰è£…OpenAIæˆ–ChromaDBå³å¯è¿è¡ŒåŸºç¡€åŠŸèƒ½")
        print("  â€¢ è§„åˆ™æ£€æµ‹å’Œé™çº§æœºåˆ¶å·¥ä½œæ­£å¸¸")
        print("  â€¢ é…ç½®OpenAI APIå¯†é’¥å¯å¯ç”¨LLMåˆ†æ")
        print("  â€¢ å®‰è£…ChromaDBå¯å¯ç”¨å®Œæ•´RAGåŠŸèƒ½")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
